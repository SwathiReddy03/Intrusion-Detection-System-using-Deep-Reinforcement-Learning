{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71803d9a",
   "metadata": {},
   "source": [
    "# 1. Building Custom Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9990c851",
   "metadata": {},
   "source": [
    "This section involves using OpenAI Gym to develop our Deep Reinforcement Learning (DRL) environment. The environment that we define will have the following characteristics:\n",
    "\n",
    "Actions:\n",
    "\n",
    "0 (do not alert)\n",
    "<br>\n",
    "1 (alert)\n",
    "\n",
    "Rewards:\n",
    "\n",
    "+1 if agent correctly alerts to an attack <br>\n",
    "0 if agent does not raise an alert when it is not needed <br>\n",
    "-1 if agent does not raise an alert when there is an attack <br>\n",
    "-1 if agent raises alert when there it is not needed <br>\n",
    "\n",
    "Episode Termination Condition:\n",
    "\n",
    "i. An episode reaches >= 500 steps <br>\n",
    "ii. An attack is issued and no alert is made <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "649c215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and filtering unnecessary tensorflow warnings\n",
    "\n",
    "import gym \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines.common.env_checker import check_env\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines.common.policies import FeedForwardPolicy\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import swifter\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # or any {'0', '1', '2'}\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=Warning)\n",
    "\n",
    "tf.get_logger().setLevel(\"INFO\")\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "969344e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRL_IDS_Env(gym.Env):\n",
    "    def __init__(self, train_data): # test data created in last module\n",
    "        '''\n",
    "        constructor\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "    \n",
    "        # set limit for episode to 500 steps\n",
    "        self.max_steps = 500\n",
    "        self.extra_steps = None # counter for steps going beyond the max_steps limit\n",
    "    \n",
    "        # defining the reward function as discussed above\n",
    "        # [(true_label, action) : reward]\n",
    "        self.rewards = {(0, 1): -1, # (benign, alert) : -1\n",
    "                        (1, 0): -1, # (attack, no alert) : -1\n",
    "                        (1, 1): 1, # (attack, alert) : 1\n",
    "                        (0, 0): 0} # (benign, no alert) : 0\n",
    "        \n",
    "        # defining action/observation space\n",
    "        self.action_space = gym.spaces.Discrete(2)  # either 0 (NORMAL) or 1 (ATTACK)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(train_data.shape[1] - 1,), dtype=np.float64) # 'box' implies we are dealing with real, valued quantities\n",
    "    \n",
    "    def step(self, action):\n",
    "        '''\n",
    "        agent taking a single step\n",
    "        this method is called after an agent takes a step\n",
    "        '''\n",
    "        \n",
    "        # check if action exists in action space\n",
    "        try:\n",
    "            self.action_space.contains(action)\n",
    "        except AssertionError as msg:\n",
    "            print(msg)\n",
    "        \n",
    "        # determine if the episode is finished\n",
    "        ep_info = {}\n",
    "        finished = False\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            ep_info['end_cause'] = 'max_step_limit_reached'\n",
    "            finished = True # we do not want to exceed our max step limit\n",
    "        \n",
    "        if self.label == 1 and action == 0: # this implies there was an attack that we did not alert\n",
    "            ep_info['end_cause'] = 'attack_unalerted'\n",
    "            finished = True\n",
    "            \n",
    "        # calculate reward based on the label of the observation and action taken by agent\n",
    "        reward = self.rewards[(self.label, action)] # maps back to our self.reward dictionary\n",
    "        \n",
    "        # calculate the next state if finished = False\n",
    "        if not finished:\n",
    "            self.i += 1 # hop to next row in dataset\n",
    "            if self.i >= self.train_data.shape[0]: # if this extends beyond the number of rows in our dataset\n",
    "                self.i = 0 # set back to first 'state'\n",
    "            \n",
    "            self.obs = self.train_data.iloc[self.i] # pulling that row, or 'observation' from our dataset\n",
    "            self.label = int(self.obs.pop('label'))\n",
    "            \n",
    "        elif self.extra_steps is None:\n",
    "            self.extra_steps = 0\n",
    "        else:\n",
    "            if self.extra_steps == 0:\n",
    "                gym.logger.warn('Episode max_step length exceeded. You are entering uncharted territory and should reset the episode.')\n",
    "                self.extra_steps += 1\n",
    "                reward = 0\n",
    "                \n",
    "        return self.obs.values, reward, finished, ep_info\n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        extra_steps = None\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.i = np.random.randint(0, self.train_data.shape[0]) # pick a random starting location from the 0th row to the nth row\n",
    "        \n",
    "        #print('reset at state number: ', self.i)\n",
    "        \n",
    "        self.obs = self.train_data.iloc[self.i]\n",
    "        \n",
    "        # record the true label of self.obs\n",
    "        self.label = int(self.obs.pop('label'))\n",
    "        \n",
    "        return self.obs.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73609ea8",
   "metadata": {},
   "source": [
    "Now we will create an instance of DRL_IDS_Env (and validate it using stable_baselines)\n",
    "Note: stable_baselines (https://stable-baselines.readthedocs.io/en/master/) is a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d2cd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"processed_data/train.csv\")\n",
    "env = DRL_IDS_Env(train_data)\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c85a10",
   "metadata": {},
   "source": [
    "# 2. Training within Custom Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b768bf7e",
   "metadata": {},
   "source": [
    "We will be training the algorithm on multiple environment in parallel through the stable-baselines lib (vectorized environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc0c2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 16  # hyperparameter\n",
    "env = DummyVecEnv([lambda: DRL_IDS_Env(train_data)] * n_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ecfbd792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining network architecture\n",
    "\n",
    "# https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html#custom-policy\n",
    "class CustomPolicy(FeedForwardPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPolicy, self).__init__(\n",
    "            *args,\n",
    "            **kwargs,\n",
    "            net_arch=[128, 64, 32],\n",
    "            act_fun=tf.nn.relu,  # Using the ReLU (REctified Linear Unit) activation function\n",
    "            feature_extraction= \"mlp\" # Multi-Layer Perceptrons\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "690635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ADJUST THESE HYPERPARAMETERS\n",
    "\n",
    "'''\n",
    "The Proximal Policy Optimization algorithm combines ideas from A2C (having multiple workers) and TRPO\n",
    "(it uses a trust region to improve the actor).\n",
    "\n",
    "The main idea is that after an update, the new policy should be not too far from the old policy. For that, PPO\n",
    "uses clipping to avoid too large of updates.\n",
    "'''\n",
    "model = PPO2(\n",
    "    CustomPolicy,\n",
    "    env,\n",
    "    gamma=0.9,\n",
    "    n_steps=512,\n",
    "    ent_coef=1e-05,\n",
    "    learning_rate=lambda progress: progress\n",
    "    * 0.0021,  # progress decreases from 1 to 0 -> lr decreasesb from 0.0021 to 0\n",
    "    vf_coef=0.6,\n",
    "    max_grad_norm=0.8,\n",
    "    lam=0.8,\n",
    "    nminibatches=16,\n",
    "    noptepochs=55,\n",
    "    cliprange=0.2,\n",
    "    verbose=0,\n",
    "    tensorboard_log=\"log_25\",  # define the tensorboard log location\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b66c5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccF1Callback(BaseCallback):\n",
    "    def __init__(self, train, val, eval_freq):\n",
    "        super().__init__()\n",
    "        self.train_data = train\n",
    "        self.val_data = val\n",
    "        self.eval_freq = eval_freq\n",
    "\n",
    "    def _on_step(self):\n",
    "        '''\n",
    "        _on_step will be called after every eval_freq steps\n",
    "        '''\n",
    "\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "\n",
    "            # calculating metrics to print for training data\n",
    "            predicted = self.train_data.drop(columns=[\"label\"]).swifter.apply(lambda x: self.model.predict(x, deterministic=True)[0], axis=1)\n",
    "            accuracy = accuracy_score(self.train_data[\"label\"], predicted)\n",
    "            f1 = f1_score(self.train_data[\"label\"], predicted)\n",
    "\n",
    "            print(\"*-\" * 50)\n",
    "            \n",
    "            print(\"total current timesteps: \", self.num_timesteps, '\\n')\n",
    "            print(\"Training --- Accuracy: \", accuracy)\n",
    "            print(\"Training --- F1-Score: \", f1, '\\n')\n",
    "\n",
    "            # calculating metrics to print for validation data\n",
    "            predicted = self.val_data.drop(columns=[\"label\"]).swifter.apply(lambda x: self.model.predict(x, deterministic=True)[0], axis=1)\n",
    "            accuracy = accuracy_score(self.val_data[\"label\"], predicted)\n",
    "            \n",
    "            f1 = f1_score(self.val_data[\"label\"], predicted)\n",
    "            print(\"Validation --- Accuracy: \", accuracy)\n",
    "            print(\"Validation --- F1-Score: \", f1)\n",
    "            \n",
    "            print(\"*-\" * 50)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f0be04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(\"processed_data/val.csv\")\n",
    "eval_callback = AccF1Callback(train_data, val_data, eval_freq=1000 // n_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe3c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "total current timesteps:  992 \n",
      "\n",
      "Training --- Accuracy:  0.9734716881050258\n",
      "Training --- F1-Score:  0.9350720655175491 \n",
      "\n",
      "Validation --- Accuracy:  0.9738518322061619\n",
      "Validation --- F1-Score:  0.9361945389730508\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "total current timesteps:  1984 \n",
      "\n",
      "Training --- Accuracy:  0.9734716881050258\n",
      "Training --- F1-Score:  0.9350720655175491 \n",
      "\n",
      "Validation --- Accuracy:  0.9738518322061619\n",
      "Validation --- F1-Score:  0.9361945389730508\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "total current timesteps:  2976 \n",
      "\n",
      "Training --- Accuracy:  0.9734716881050258\n",
      "Training --- F1-Score:  0.9350720655175491 \n",
      "\n",
      "Validation --- Accuracy:  0.9738518322061619\n",
      "Validation --- F1-Score:  0.9361945389730508\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "total current timesteps:  3968 \n",
      "\n",
      "Training --- Accuracy:  0.9734716881050258\n",
      "Training --- F1-Score:  0.9350720655175491 \n",
      "\n",
      "Validation --- Accuracy:  0.9738518322061619\n",
      "Validation --- F1-Score:  0.9361945389730508\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "total current timesteps:  4960 \n",
      "\n",
      "Training --- Accuracy:  0.9734716881050258\n",
      "Training --- F1-Score:  0.9350720655175491 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.learn(5000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304d0584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d0bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
